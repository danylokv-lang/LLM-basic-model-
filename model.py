"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                     GPT –ú–û–î–ï–õ–¨                              ‚ïë
‚ïë                                                              ‚ïë
‚ïë  –ü–æ–≤–Ω–∞ GPT-style –º–æ–≤–Ω–∞ –º–æ–¥–µ–ª—å (decoder-only Transformer).   ‚ïë
‚ïë                                                              ‚ïë
‚ïë  –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞:                                                ‚ïë
‚ïë                                                              ‚ïë
‚ïë  –í—Ö—ñ–¥–Ω—ñ —Ç–æ–∫–µ–Ω–∏ [B, T]                                       ‚ïë
‚ïë       ‚îÇ                                                      ‚ïë
‚ïë       ‚îú‚îÄ‚ñ∫ Token Embedding (vocab ‚Üí n_embed)                 ‚ïë
‚ïë       ‚îú‚îÄ‚ñ∫ Position Embedding (position ‚Üí n_embed)           ‚ïë
‚ïë       ‚îÇ         ‚îÇ                                            ‚ïë
‚ïë       ‚îÇ    –°—É–º–∞ + Dropout                                   ‚ïë
‚ïë       ‚îÇ         ‚îÇ                                            ‚ïë
‚ïë       ‚îÇ    N √ó TransformerBlock:                             ‚ïë
‚ïë       ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚ïë
‚ïë       ‚îÇ    ‚îÇ LayerNorm ‚Üí Attention    ‚îÇ                      ‚ïë
‚ïë       ‚îÇ    ‚îÇ + Residual Connection    ‚îÇ                      ‚ïë
‚ïë       ‚îÇ    ‚îÇ LayerNorm ‚Üí FFN          ‚îÇ                      ‚ïë
‚ïë       ‚îÇ    ‚îÇ + Residual Connection    ‚îÇ                      ‚ïë
‚ïë       ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚ïë
‚ïë       ‚îÇ         ‚îÇ                                            ‚ïë
‚ïë       ‚îÇ    Final LayerNorm                                   ‚ïë
‚ïë       ‚îÇ         ‚îÇ                                            ‚ïë
‚ïë       ‚îÇ    Linear Head ‚Üí logits [B, T, vocab_size]          ‚ïë
‚ïë       ‚ñº                                                      ‚ïë
‚ïë  Loss = CrossEntropy(logits, targets)                       ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass

from multihead import CausalSelfAttention


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø –ú–û–î–ï–õ–Ü
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass
class GPTConfig:
    """
    Dataclass –∑ —É—Å—ñ–º–∞ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –º–æ–¥–µ–ª—ñ.

    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ dataclass –∑–∞–º—ñ—Å—Ç—å —Å–ª–æ–≤–Ω–∏–∫–∞ ‚Äî —Ü–µ –¥–∞—î:
    - –ê–≤—Ç–æ–¥–æ–ø–æ–≤–Ω–µ–Ω–Ω—è –≤ IDE
    - –ü–µ—Ä–µ–≤—ñ—Ä–∫—É —Ç–∏–ø—ñ–≤
    - –ó—Ä—É—á–Ω–∏–π __repr__ –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
    """
    vocab_size: int = 256       # –†–æ–∑–º—ñ—Ä —Å–ª–æ–≤–Ω–∏–∫–∞ (–≤—Å—Ç–∞–Ω–æ–≤–ª—é—î—Ç—å—Å—è —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä–æ–º)
    block_size: int = 256       # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
    n_embed: int = 128          # –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤
    n_heads: int = 4            # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –≥–æ–ª—ñ–≤ —É–≤–∞–≥–∏
    n_layers: int = 4           # –ö—ñ–ª—å–∫—ñ—Å—Ç—å Transformer –±–ª–æ–∫—ñ–≤
    dropout: float = 0.1        # –ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å Dropout

    def __post_init__(self):
        """–í–∞–ª—ñ–¥–∞—Ü—ñ—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –ø—Ä–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ –∫–æ–Ω—Ñ—ñ–≥—É."""
        assert self.n_embed % self.n_heads == 0, \
            f"n_embed ({self.n_embed}) –º–∞—î –¥—ñ–ª–∏—Ç–∏—Å—è –Ω–∞ n_heads ({self.n_heads})"


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FEEDFORWARD NETWORK (MLP)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class FeedForward(nn.Module):
    """
    Position-wise Feed-Forward Network.

    –¶–µ –¥–≤–æ—à–∞—Ä–æ–≤–∏–π MLP (Multi-Layer Perceptron) —è–∫–∏–π –æ–±—Ä–æ–±–ª—è—î
    –∫–æ–∂–Ω—É –ø–æ–∑–∏—Ü—ñ—é –ù–ï–ó–ê–õ–ï–ñ–ù–û (—Ç–æ–º—É "position-wise").

    –°—Ç—Ä—É–∫—Ç—É—Ä–∞:
      Linear(n_embed ‚Üí 4*n_embed)    # –†–æ–∑—à–∏—Ä–µ–Ω–Ω—è
      GELU()                          # –ê–∫—Ç–∏–≤–∞—Ü—ñ—è
      Linear(4*n_embed ‚Üí n_embed)    # –°—Ç–∏—Å–Ω–µ–Ω–Ω—è –Ω–∞–∑–∞–¥
      Dropout()                       # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è

    –ß–æ–º—É 4x —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è?
      –¶–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç –∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ—ó —Å—Ç–∞—Ç—Ç—ñ "Attention Is All You Need".
      –†–æ–∑—à–∏—Ä–µ–Ω–Ω—è –¥–∞—î –Ω–µ–π—Ä–æ–Ω–Ω—ñ–π –º–µ—Ä–µ–∂—ñ –±—ñ–ª—å—à–µ "–ø—Ä–æ—Å—Ç–æ—Ä—É" –¥–ª—è
      –æ–±—á–∏—Å–ª–µ–Ω—å, –∞ –ø–æ—Ç—ñ–º –º–∏ —Å—Ç–∏—Å–∫–∞—î–º–æ –Ω–∞–∑–∞–¥.

    –ß–æ–º—É GELU –∑–∞–º—ñ—Å—Ç—å ReLU?
      GELU (Gaussian Error Linear Unit) = x ¬∑ Œ¶(x)
      –¥–µ Œ¶ ‚Äî CDF —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–æ–∑–ø–æ–¥—ñ–ª—É.

      –ù–∞ –≤—ñ–¥–º—ñ–Ω—É –≤—ñ–¥ ReLU (max(0, x)), GELU ‚Äî –ø–ª–∞–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è.
      ReLU —Ä—ñ–∑–∫–æ "–æ–±—Ä—ñ–∑–∞—î" –≤—ñ–¥'—î–º–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è, –∞ GELU ‚Äî –ø–ª–∞–≤–Ω–æ
      –∑–º–µ–Ω—à—É—î —ó—Ö. –¶–µ –¥–∞—î –∫—Ä–∞—â—ñ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ —ñ –∫—Ä–∞—â—É –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü—ñ—é.

      –í—Å—ñ —Å—É—á–∞—Å–Ω—ñ LLM (GPT, BERT, LLaMA) –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å GELU.
    """

    def __init__(self, config):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(config.n_embed, 4 * config.n_embed),
            nn.GELU(),      # ‚Üê –ü–ª–∞–≤–Ω–∞ –∞–∫—Ç–∏–≤–∞—Ü—ñ—è –∑–∞–º—ñ—Å—Ç—å ReLU
            nn.Linear(4 * config.n_embed, config.n_embed),
            nn.Dropout(config.dropout),
        )

    def forward(self, x):
        return self.net(x)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TRANSFORMER BLOCK
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class TransformerBlock(nn.Module):
    """
    –û–¥–∏–Ω –±–ª–æ–∫ Transformer-–∞.

    –ö–æ–∂–µ–Ω –±–ª–æ–∫ —Å–∫–ª–∞–¥–∞—î—Ç—å—Å—è –∑ –¥–≤–æ—Ö –ø—ñ–¥—à–∞—Ä—ñ–≤:
    1. Multi-Head Self-Attention ‚Äî –¥–æ–∑–≤–æ–ª—è—î —Ç–æ–∫–µ–Ω–∞–º "—Å–ø—ñ–ª–∫—É–≤–∞—Ç–∏—Å—è"
    2. Feed-Forward Network ‚Äî "–æ–±–¥—É–º—É—î" —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é

    –ö–ª—é—á–æ–≤—ñ —Ç–µ—Ö–Ω—ñ–∫–∏:

    ‚îÄ‚îÄ Pre-Layer Normalization (Pre-LN) ‚îÄ‚îÄ
    –ú–∏ –Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –ü–ï–†–ï–î –∫–æ–∂–Ω–∏–º –ø—ñ–¥—à–∞—Ä–æ–º (–∞ –Ω–µ –ø—ñ—Å–ª—è, —è–∫ –≤
    –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ–π —Å—Ç–∞—Ç—Ç—ñ). GPT-2 –ø–æ–∫–∞–∑–∞–≤, —â–æ Pre-LN –∑–Ω–∞—á–Ω–æ
    —Å—Ç–∞–±—ñ–ª—ñ–∑—É—î —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –≥–ª–∏–±–æ–∫–∏—Ö –º–µ—Ä–µ–∂.

    LayerNorm –Ω–æ—Ä–º–∞–ª—ñ–∑—É—î –∫–æ–∂–µ–Ω –≤–µ–∫—Ç–æ—Ä —Ç–∞–∫, —â–æ–± —Å–µ—Ä–µ–¥–Ω—î = 0,
    –¥–∏—Å–ø–µ—Ä—Å—ñ—è = 1. –¶–µ –∑–∞–ø–æ–±—ñ–≥–∞—î "–≤–∏–±—É—Ö—É" –∞–±–æ "–∑–Ω–∏–∫–Ω–µ–Ω–Ω—é"
    –∑–Ω–∞—á–µ–Ω—å –º—ñ–∂ —à–∞—Ä–∞–º–∏.

    ‚îÄ‚îÄ Residual Connection (–∑–∞–ª–∏—à–∫–æ–≤–∏–π –∑–≤'—è–∑–æ–∫) ‚îÄ‚îÄ
    output = input + sublayer(norm(input))

    –ó–∞–º—ñ—Å—Ç—å: output = sublayer(input)
    –ú–∏:      output = input + sublayer(input)

    –¶–µ –¥–æ–∑–≤–æ–ª—è—î –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞–º "—Ç–µ–∫—Ç–∏" –Ω–∞–ø—Ä—è–º—É –≤—ñ–¥ –≤–∏—Ö–æ–¥—É –¥–æ –≤—Ö–æ–¥—É,
    –æ–º–∏–Ω–∞—é—á–∏ —Å–∫–ª–∞–¥–Ω—ñ –ø—ñ–¥—à–∞—Ä–∏. –ë–µ–∑ —Ü—å–æ–≥–æ –≥–ª–∏–±–æ–∫—ñ –º–µ—Ä–µ–∂—ñ (>6 —à–∞—Ä—ñ–≤)
    –ø—Ä–∞–∫—Ç–∏—á–Ω–æ –Ω–µ —Ç—Ä–µ–Ω—É—é—Ç—å—Å—è ‚Äî –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ –∑–Ω–∏–∫–∞—é—Ç—å.
    """

    def __init__(self, config):
        super().__init__()

        # Pre-LayerNorm –¥–ª—è attention
        self.ln1 = nn.LayerNorm(config.n_embed)

        # Multi-Head Self-Attention
        self.attn = CausalSelfAttention(
            n_embed=config.n_embed,
            n_heads=config.n_heads,
            block_size=config.block_size,
            dropout=config.dropout,
        )

        # Pre-LayerNorm –¥–ª—è FFN
        self.ln2 = nn.LayerNorm(config.n_embed)

        # Feed-Forward Network
        self.ffn = FeedForward(config)

    def forward(self, x):
        """
        –ü—Ä—è–º–∏–π –ø—Ä–æ—Ö—ñ–¥:
          x ‚Üí LayerNorm ‚Üí Attention ‚Üí +Residual
            ‚Üí LayerNorm ‚Üí FFN ‚Üí +Residual
        """
        # Attention –∑ residual connection
        # x + attn(ln1(x)): –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ attn "–∑–ª–∞–º–∞—î—Ç—å—Å—è", x –∑–∞–ª–∏—à–∏—Ç—å—Å—è
        x = x + self.attn(self.ln1(x))

        # FFN –∑ residual connection
        x = x + self.ffn(self.ln2(x))

        return x


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ü–û–í–ù–ê GPT –ú–û–î–ï–õ–¨
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class GPT(nn.Module):
    """
    GPT (Generative Pre-trained Transformer) –º–æ–≤–Ω–∞ –º–æ–¥–µ–ª—å.

    –¶–µ decoder-only Transformer, —â–æ –≤—á–∏—Ç—å—Å—è –ø–µ—Ä–µ–¥–±–∞—á–∞—Ç–∏
    –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Ç–æ–∫–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö.

    –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∏:
    1. Token Embedding ‚Äî –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î ID —Ç–æ–∫–µ–Ω—ñ–≤ —É –≤–µ–∫—Ç–æ—Ä–∏
    2. Position Embedding ‚Äî –∫–æ–¥—É—î –ø–æ–∑–∏—Ü—ñ—é –∫–æ–∂–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
    3. N √ó TransformerBlock ‚Äî –æ—Å–Ω–æ–≤–Ω–µ "–º–∏—Å–ª–µ–Ω–Ω—è" –º–æ–¥–µ–ª—ñ
    4. Final LayerNorm ‚Äî —Ñ—ñ–Ω–∞–ª—å–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
    5. LM Head ‚Äî –ø—Ä–æ–µ–∫—Ü—ñ—è –Ω–∞–∑–∞–¥ —É —Å–ª–æ–≤–Ω–∏–∫ (–¥–ª—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è)
    """

    def __init__(self, config):
        super().__init__()
        self.config = config

        # ‚îÄ‚îÄ‚îÄ Token Embedding ‚îÄ‚îÄ‚îÄ
        # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î ID —Ç–æ–∫–µ–Ω–∞ –≤ –≥—É—Å—Ç–∏–π –≤–µ–∫—Ç–æ—Ä —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ n_embed.
        # –ö–æ–∂–µ–Ω —Ç–æ–∫–µ–Ω –æ—Ç—Ä–∏–º—É—î —Å–≤—ñ–π —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π –≤–µ–∫—Ç–æ—Ä, —è–∫–∏–π
        # —Ç—Ä–µ–Ω—É—î—Ç—å—Å—è —Ä–∞–∑–æ–º –∑ –º–æ–¥–µ–ª–ª—é.
        #
        # –ù–∞–ø—Ä–∏–∫–ª–∞–¥, "–ø—Ä–∏–≤—ñ—Ç" (id=42) ‚Üí [0.3, -0.1, 0.8, ...]
        # –°–µ–º–∞–Ω—Ç–∏—á–Ω–æ –±–ª–∏–∑—å–∫—ñ —Ç–æ–∫–µ–Ω–∏ –æ—Ç—Ä–∏–º–∞—é—Ç—å –±–ª–∏–∑—å–∫—ñ –≤–µ–∫—Ç–æ—Ä–∏.
        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embed)

        # ‚îÄ‚îÄ‚îÄ Position Embedding ‚îÄ‚îÄ‚îÄ
        # Transformer —Å–∞–º –ø–æ —Å–æ–±—ñ –ù–ï –∑–Ω–∞—î –ø–æ—Ä—è–¥–æ–∫ —Ç–æ–∫–µ–Ω—ñ–≤!
        # (–±–æ attention –æ–±—Ä–æ–±–ª—è—î –≤—Å—ñ –ø–æ–∑–∏—Ü—ñ—ó "–ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ")
        #
        # Position Embedding –¥–æ–¥–∞—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –ø–æ–∑–∏—Ü—ñ—é:
        # –ø–æ–∑–∏—Ü—ñ—è 0 ‚Üí [0.1, 0.2, ...], –ø–æ–∑–∏—Ü—ñ—è 1 ‚Üí [-0.1, 0.3, ...], ...
        #
        # –ë–µ–∑ —Ü—å–æ–≥–æ –º–æ–¥–µ–ª—å –Ω–µ —Ä–æ–∑—Ä—ñ–∑–Ω–∏—Ç—å "–∫—ñ—Ç —ó—Å—Ç—å —Ä–∏–±—É" —Ç–∞ "—Ä–∏–±—É —ó—Å—Ç—å –∫—ñ—Ç"
        self.pos_emb = nn.Embedding(config.block_size, config.n_embed)

        # ‚îÄ‚îÄ‚îÄ Dropout –Ω–∞ –µ–º–±–µ–¥–∏–Ω–≥–∞—Ö ‚îÄ‚îÄ‚îÄ
        self.drop = nn.Dropout(config.dropout)

        # ‚îÄ‚îÄ‚îÄ –°—Ç–µ–∫ Transformer –±–ª–æ–∫—ñ–≤ ‚îÄ‚îÄ‚îÄ
        # nn.ModuleList ‚Äî —Å–ø–∏—Å–æ–∫ –º–æ–¥—É–ª—ñ–≤, —è–∫—ñ PyTorch "–±–∞—á–∏—Ç—å"
        # —ñ –≤–∫–ª—é—á–∞—î –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ (–Ω–∞ –≤—ñ–¥–º—ñ–Ω—É –≤—ñ–¥ –∑–≤–∏—á–∞–π–Ω–æ–≥–æ list)
        self.blocks = nn.ModuleList([
            TransformerBlock(config) for _ in range(config.n_layers)
        ])

        # ‚îÄ‚îÄ‚îÄ –§—ñ–Ω–∞–ª—å–Ω–∏–π LayerNorm ‚îÄ‚îÄ‚îÄ
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–µ—Ä–µ–¥ —Ñ—ñ–Ω–∞–ª—å–Ω–æ—é –ø—Ä–æ–µ–∫—Ü—ñ—î—é ‚Äî —Å—Ç–∞–±—ñ–ª—ñ–∑—É—î –≤–∏—Ö—ñ–¥
        self.ln_final = nn.LayerNorm(config.n_embed)

        # ‚îÄ‚îÄ‚îÄ Language Model Head ‚îÄ‚îÄ‚îÄ
        # –ü—Ä–æ–µ–∫—Ç—É—î –µ–º–±–µ–¥–∏–Ω–≥ –Ω–∞–∑–∞–¥ —É —Ä–æ–∑–º—ñ—Ä —Å–ª–æ–≤–Ω–∏–∫–∞.
        # –í–∏—Ö—ñ–¥ ‚Äî logits (–Ω–µ–Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –æ—Ü—ñ–Ω–∫–∏) –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ —Å–ª–æ–≤–Ω–∏–∫–∞.
        # softmax(logits) –¥–∞—î –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞.
        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)

        # ‚îÄ‚îÄ‚îÄ Weight Tying ‚îÄ‚îÄ‚îÄ
        # –†–æ–∑–¥—ñ–ª—è—î–º–æ –≤–∞–≥–∏ –º—ñ–∂ Token Embedding —ñ LM Head.
        # –Ü–¥–µ—è: —è–∫—â–æ –¥–≤–∞ —Å–ª–æ–≤–∞ –º–∞—é—Ç—å —Å—Ö–æ–∂—ñ –µ–º–±–µ–¥–∏–Ω–≥–∏, —Ç–æ
        # –º–æ–¥–µ–ª—å –º–∞—î —ñ –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —ó—Ö –∑ –ø–æ–¥—ñ–±–Ω–æ—é –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—é.
        # –¶–µ –∑–º–µ–Ω—à—É—î –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ —ñ –ø–æ–∫—Ä–∞—â—É—î —è–∫—ñ—Å—Ç—å.
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –≤ GPT-2, LLaMA —Ç–∞ –±–∞–≥–∞—Ç—å–æ—Ö —ñ–Ω—à–∏—Ö.
        self.tok_emb.weight = self.lm_head.weight

        # ‚îÄ‚îÄ‚îÄ –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –≤–∞–≥ ‚îÄ‚îÄ‚îÄ
        self.apply(self._init_weights)
        print(f"üß† GPT –º–æ–¥–µ–ª—å: {self.count_parameters():.2f}M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤")

    def _init_weights(self, module):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –≤–∞–≥ –Ω–µ–π—Ä–æ–Ω–Ω–æ—ó –º–µ—Ä–µ–∂—ñ.

        –ü—Ä–∞–≤–∏–ª—å–Ω–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ö–†–ò–¢–ò–ß–ù–ê –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è:
        - –ó–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫—ñ –≤–∞–≥–∏ ‚Üí –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ "–≤–∏–±—É—Ö–∞—é—Ç—å"
        - –ó–∞–Ω–∞–¥—Ç–æ –º–∞–ª—ñ –≤–∞–≥–∏ ‚Üí –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ "–∑–Ω–∏–∫–∞—é—Ç—å"

        –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ Normal(0, 0.02) ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è GPT –º–æ–¥–µ–ª–µ–π.
        –ï–º–±–µ–¥–∏–Ω–≥–∏ —Ç–∞ LayerNorm —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—é—Ç—å—Å—è –æ–∫—Ä–µ–º–æ.
        """
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def count_parameters(self):
        """–†–∞—Ö—É—î –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç—Ä–µ–Ω–æ–≤–∞–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ —É –º—ñ–ª—å–π–æ–Ω–∞—Ö."""
        return sum(p.numel() for p in self.parameters() if p.requires_grad) / 1e6

    def forward(self, idx, targets=None):
        """
        –ü—Ä—è–º–∏–π –ø—Ä–æ—Ö—ñ–¥ GPT –º–æ–¥–µ–ª—ñ.

        Args:
            idx:     –Ü–Ω–¥–µ–∫—Å–∏ —Ç–æ–∫–µ–Ω—ñ–≤ (B, T) ‚Äî batch √ó sequence length
            targets: –¶—ñ–ª—å–æ–≤—ñ —Ç–æ–∫–µ–Ω–∏ (B, T) ‚Äî –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è loss

        Returns:
            logits: –ù–µ–Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –æ—Ü—ñ–Ω–∫–∏ (B, T, vocab_size)
            loss:   Cross-entropy loss (–∞–±–æ None —è–∫—â–æ targets –Ω–µ –∑–∞–¥–∞–Ω—ñ)
        """
        B, T = idx.shape
        device = idx.device

        assert T <= self.config.block_size, \
            f"–ü–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å ({T}) –¥–æ–≤—à–∞ –∑–∞ block_size ({self.config.block_size})"

        # ‚îÄ‚îÄ‚îÄ –ö—Ä–æ–∫ 1: –ï–º–±–µ–¥–∏–Ω–≥–∏ ‚îÄ‚îÄ‚îÄ
        # –ö–æ–∂–µ–Ω —Ç–æ–∫–µ–Ω ‚Üí –≤–µ–∫—Ç–æ—Ä, –∫–æ–∂–Ω–∞ –ø–æ–∑–∏—Ü—ñ—è ‚Üí –≤–µ–∫—Ç–æ—Ä, —Å—É–º—É—î–º–æ
        pos = torch.arange(0, T, dtype=torch.long, device=device)  # [0, 1, 2, ..., T-1]

        tok_emb = self.tok_emb(idx)    # (B, T, n_embed) ‚Äî –µ–º–±–µ–¥–∏–Ω–≥–∏ —Ç–æ–∫–µ–Ω—ñ–≤
        pos_emb = self.pos_emb(pos)    # (T, n_embed) ‚Üí broadcast –¥–æ (B, T, n_embed)

        x = self.drop(tok_emb + pos_emb)

        # ‚îÄ‚îÄ‚îÄ –ö—Ä–æ–∫ 2: Transformer –±–ª–æ–∫–∏ ‚îÄ‚îÄ‚îÄ
        # –ü–æ—Å–ª—ñ–¥–æ–≤–Ω–æ –ø—Ä–æ—Ö–æ–¥–∏–º–æ —á–µ—Ä–µ–∑ –≤—Å—ñ N –±–ª–æ–∫—ñ–≤
        for block in self.blocks:
            x = block(x)

        # ‚îÄ‚îÄ‚îÄ –ö—Ä–æ–∫ 3: –§—ñ–Ω–∞–ª—å–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è + –ø—Ä–æ–µ–∫—Ü—ñ—è ‚îÄ‚îÄ‚îÄ
        x = self.ln_final(x)
        logits = self.lm_head(x)       # (B, T, vocab_size)

        # ‚îÄ‚îÄ‚îÄ –ö—Ä–æ–∫ 4: –û–±—á–∏—Å–ª–µ–Ω–Ω—è Loss ‚îÄ‚îÄ‚îÄ
        loss = None
        if targets is not None:
            # Cross-entropy loss:
            # –ü–æ—Ä—ñ–≤–Ω—é—î –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—ñ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –∑ —Ä–µ–∞–ª—å–Ω–∏–º –Ω–∞—Å—Ç—É–ø–Ω–∏–º —Ç–æ–∫–µ–Ω–æ–º.
            # loss = -log(P(–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ç–æ–∫–µ–Ω))
            # –ß–∏–º –≤–∏—â–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞, —Ç–∏–º –º–µ–Ω—à–∏–π loss.
            logits_flat = logits.view(-1, logits.size(-1))
            targets_flat = targets.view(-1)
            loss = F.cross_entropy(logits_flat, targets_flat)

        return logits, loss

    @torch.no_grad()
    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, top_p=None):
        """
        –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É (–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å–∏–≤–Ω–∞).

        –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä—É—î –ø–æ –û–î–ù–û–ú–£ —Ç–æ–∫–µ–Ω—É –∑–∞ —Ä–∞–∑:
        1. –ü–æ–¥–∞—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚Üí –æ—Ç—Ä–∏–º—É—î–º–æ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
        2. –û–±–∏—Ä–∞—î–º–æ —Ç–æ–∫–µ–Ω (sampling) ‚Üí –¥–æ–¥–∞—î–º–æ –¥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
        3. –ü–æ–≤—Ç–æ—Ä—é—î–º–æ

        Args:
            idx:             –ü–æ—á–∞—Ç–∫–æ–≤–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (B, T)
            max_new_tokens:  –°–∫—ñ–ª—å–∫–∏ —Ç–æ–∫–µ–Ω—ñ–≤ –∑–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏
            temperature:     –ö–æ–Ω—Ç—Ä–æ–ª—å "–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—ñ":
                            < 1.0 ‚Üí –≤–ø–µ–≤–Ω–µ–Ω—ñ—à–∏–π, –ø–æ–≤—Ç–æ—Ä—é–≤–∞–Ω—ñ—à–∏–π —Ç–µ–∫—Å—Ç
                            > 1.0 ‚Üí —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—à–∏–π, –º–µ–Ω—à –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–∏–π
                            = 1.0 ‚Üí —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª
            top_k:           –û–±–∏—Ä–∞—Ç–∏ –ª–∏—à–µ –∑ K –Ω–∞–π—ñ–º–æ–≤—ñ—Ä–Ω—ñ—à–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤.
                            top_k=10 ‚Üí —Ä–æ–∑–≥–ª—è–¥–∞—î–º–æ –ª–∏—à–µ 10 –Ω–∞–π–∫—Ä–∞—â–∏—Ö.
                            –ó–º–µ–Ω—à—É—î "—Å–º—ñ—Ç—Ç—è" —É –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó.
            top_p:           Nucleus sampling ‚Äî –æ–±–∏—Ä–∞—Ç–∏ –∑ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–æ—ó –º–Ω–æ–∂–∏–Ω–∏
                            —Ç–æ–∫–µ–Ω—ñ–≤, —á–∏—è —Å—É–º–∞—Ä–Ω–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å ‚â• p.
                            top_p=0.9 ‚Üí –±–µ—Ä–µ–º–æ —Ç–æ–ø-—Ç–æ–∫–µ–Ω–∏ —â–æ –¥–∞—é—Ç—å 90%.
                            –ê–¥–∞–ø—Ç–∏–≤–Ω—ñ—à–µ –∑–∞ top_k.

        Returns:
            idx: –†–æ–∑—à–∏—Ä–µ–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑ –Ω–æ–≤–∏–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏ (B, T + max_new_tokens)
        """
        self.eval()  # –í–∏–º–∏–∫–∞—î–º–æ Dropout

        for _ in range(max_new_tokens):
            # –û–±—Ä—ñ–∑–∞—î–º–æ –¥–æ block_size (–º–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ –æ–±—Ä–æ–±–∏—Ç–∏ –¥–æ–≤—à—É –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å)
            idx_cond = idx[:, -self.config.block_size:]

            # –û—Ç—Ä–∏–º—É—î–º–æ logits –¥–ª—è –æ—Å—Ç–∞–Ω–Ω—å–æ—ó –ø–æ–∑–∏—Ü—ñ—ó
            logits, _ = self(idx_cond)
            logits = logits[:, -1, :]  # –ù–∞—Å —Ü—ñ–∫–∞–≤–∏—Ç—å –ª–∏—à–µ –æ—Å—Ç–∞–Ω–Ω—ñ–π —Ç–æ–∫–µ–Ω

            # ‚îÄ‚îÄ‚îÄ Temperature Scaling ‚îÄ‚îÄ‚îÄ
            # –î—ñ–ª–∏–º–æ logits –Ω–∞ temperature –ø–µ—Ä–µ–¥ softmax:
            # T < 1: —Ä–æ–±–∏—Ç—å —Ä–æ–∑–ø–æ–¥—ñ–ª "–≥–æ—Å—Ç—Ä—ñ—à–∏–º" (–±—ñ–ª—å—à–µ –≤–ø–µ–≤–Ω–µ–Ω–æ—Å—Ç—ñ)
            # T > 1: —Ä–æ–±–∏—Ç—å —Ä–æ–∑–ø–æ–¥—ñ–ª "–ø–ª–∞—Å–∫–∏–º" (–±—ñ–ª—å—à–µ –≤–∏–ø–∞–¥–∫–æ–≤–æ—Å—Ç—ñ)
            if temperature != 1.0:
                logits = logits / temperature

            # ‚îÄ‚îÄ‚îÄ Top-K Filtering ‚îÄ‚îÄ‚îÄ
            # –ó–∞–ª–∏—à–∞—î–º–æ –ª–∏—à–µ K –Ω–∞–π—ñ–º–æ–≤—ñ—Ä–Ω—ñ—à–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤,
            # —Ä–µ—à—Ç—É –∑–∞–º—ñ–Ω—é—î–º–æ –Ω–∞ -‚àû (–π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å = 0 –ø—ñ—Å–ª—è softmax)
            if top_k is not None:
                top_k_val = min(top_k, logits.size(-1))
                # topk –ø–æ–≤–µ—Ä—Ç–∞—î (values, indices) ‚Äî –Ω–∞–º –ø–æ—Ç—Ä—ñ–±–Ω–µ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è
                threshold = torch.topk(logits, top_k_val).values[:, -1:]
                logits[logits < threshold] = float('-inf')

            # ‚îÄ‚îÄ‚îÄ Top-P (Nucleus) Filtering ‚îÄ‚îÄ‚îÄ
            # –°–æ—Ä—Ç—É—î–º–æ —Ç–æ–∫–µ–Ω–∏ –∑–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—é, –æ–±–∏—Ä–∞—î–º–æ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –Ω–∞–±—ñ—Ä
            # –∑ —Å—É–º–∞—Ä–Ω–æ—é –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—é ‚â• p
            if top_p is not None:
                sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                probs_sorted = F.softmax(sorted_logits, dim=-1)
                cumulative_probs = torch.cumsum(probs_sorted, dim=-1)

                # –í–∏–¥–∞–ª—è—î–º–æ —Ç–æ–∫–µ–Ω–∏ –∑ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ—é –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—é > p
                # –ó—Å—É–≤–∞—î–º–æ –Ω–∞ 1 –≤–ø—Ä–∞–≤–æ, —â–æ–± –∑–±–µ—Ä–µ–≥—Ç–∏ –ø–µ—Ä—à–∏–π —Ç–æ–∫–µ–Ω
                sorted_mask = cumulative_probs - probs_sorted > top_p
                sorted_logits[sorted_mask] = float('-inf')

                # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –Ω–∞ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ –ø–æ–∑–∏—Ü—ñ—ó
                logits = sorted_logits.scatter(1, sorted_indices, sorted_logits)

            # Softmax ‚Üí –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
            probs = F.softmax(logits, dim=-1)

            # –í–∏–ø–∞–¥–∫–æ–≤–æ –æ–±–∏—Ä–∞—î–º–æ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Ç–æ–∫–µ–Ω –∑–≥—ñ–¥–Ω–æ –∑ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—è–º–∏
            next_token = torch.multinomial(probs, num_samples=1)

            # –î–æ–¥–∞—î–º–æ –¥–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
            idx = torch.cat([idx, next_token], dim=1)

        return idx
