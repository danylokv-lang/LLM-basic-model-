"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BPE Ğ¢ĞĞšĞ•ĞĞ†Ğ—ĞĞ¢ĞĞ                            â•‘
â•‘                                                              â•‘
â•‘  Byte-Pair Encoding â€” ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ·Ğ°Ñ†Ñ–Ñ—       â•‘
â•‘  Ñƒ GPT, LLaMA Ñ‚Ğ° Ñ–Ğ½ÑˆĞ¸Ñ… LLM Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….                        â•‘
â•‘                                                              â•‘
â•‘  Ğ¯Ğº Ğ¿Ñ€Ğ°Ñ†ÑÑ” BPE:                                              â•‘
â•‘  1. Ğ Ğ¾Ğ·Ğ±Ğ¸Ğ²Ğ°Ñ”Ğ¼Ğ¾ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° Ğ¾ĞºÑ€ĞµĞ¼Ñ– ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸ (Ğ¿Ğ¾Ñ‡Ğ°Ñ‚ĞºĞ¾Ğ²Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ²Ğ½Ğ¸Ğº) â•‘
â•‘  2. Ğ—Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ğ½Ğ°Ğ¹Ñ‡Ğ°ÑÑ‚Ñ–ÑˆÑƒ Ğ¿Ğ°Ñ€Ñƒ ÑÑƒÑÑ–Ğ´Ğ½Ñ–Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ²              â•‘
â•‘  3. Ğ—Ğ»Ğ¸Ğ²Ğ°Ñ”Ğ¼Ğ¾ Ñ†Ñ Ğ¿Ğ°Ñ€Ñƒ Ğ² Ğ¾Ğ´Ğ¸Ğ½ Ğ½Ğ¾Ğ²Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½                     â•‘
â•‘  4. ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑ”Ğ¼Ğ¾ ĞºÑ€Ğ¾ĞºĞ¸ 2-3 Ğ¿Ğ¾ĞºĞ¸ ÑĞ»Ğ¾Ğ²Ğ½Ğ¸Ğº Ğ½Ğµ Ğ´Ğ¾ÑÑĞ³Ğ½Ğµ Ğ¿Ğ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¾Ğ³Ğ¾ â•‘
â•‘     Ñ€Ğ¾Ğ·Ğ¼Ñ–Ñ€Ñƒ                                                  â•‘
â•‘                                                              â•‘
â•‘  ĞŸÑ€Ğ¸ĞºĞ»Ğ°Ğ´: "Ğ°Ğ°Ğ± Ğ°Ğ°Ğ±" â†’ 'Ğ°'+'Ğ°'='Ğ°Ğ°' â†’ 'Ğ°Ğ°'+'Ğ±'='Ğ°Ğ°Ğ±'       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import json
import os


class BPETokenizer:
    """
    Byte-Pair Encoding Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ·Ğ°Ñ‚Ğ¾Ñ€.

    Ğ¡Ğ¿ĞµÑ†Ñ–Ğ°Ğ»ÑŒĞ½Ñ– Ñ‚Ğ¾ĞºĞµĞ½Ğ¸:
      <PAD> = 0  â€” padding (Ğ·Ğ°Ğ¿Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ Ğ´Ğ¾ Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾Ñ— Ğ´Ğ¾Ğ²Ğ¶Ğ¸Ğ½Ğ¸)
      <UNK> = 1  â€” Ğ½ĞµĞ²Ñ–Ğ´Ğ¾Ğ¼Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½
      <BOS> = 2  â€” Ğ¿Ğ¾Ñ‡Ğ°Ñ‚Ğ¾Ğº Ğ¿Ğ¾ÑĞ»Ñ–Ğ´Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ñ– (Begin Of Sequence)
      <EOS> = 3  â€” ĞºÑ–Ğ½ĞµÑ†ÑŒ Ğ¿Ğ¾ÑĞ»Ñ–Ğ´Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ñ– (End Of Sequence)
    """

    def __init__(self):
        # â”€â”€â”€ Ğ¡Ğ¿ĞµÑ†Ñ–Ğ°Ğ»ÑŒĞ½Ñ– Ñ‚Ğ¾ĞºĞµĞ½Ğ¸ â”€â”€â”€
        # PAD - Ğ²Ğ¸Ñ€Ñ–Ğ²Ğ½ÑÑ” Ğ¿Ğ¾ÑĞ»Ñ–Ğ´Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ñ– Ğ´Ğ¾ Ğ¾Ğ´Ğ½Ñ–Ñ”Ñ— Ğ´Ğ¾Ğ²Ğ¶Ğ¸Ğ½Ğ¸ Ğ² batch
        # UNK - Ğ·Ğ°Ğ¼Ñ–Ğ½ÑÑ” ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸ ÑĞºĞ¸Ñ… Ğ½ĞµĞ¼Ğ°Ñ” Ñƒ ÑĞ»Ğ¾Ğ²Ğ½Ğ¸ĞºÑƒ
        # BOS - ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ¼Ğ¾Ğ´ĞµĞ»Ñ– Ñ‰Ğ¾ Ğ¿Ğ¾Ñ‡Ğ¸Ğ½Ğ°Ñ”Ñ‚ÑŒÑÑ Ğ½Ğ¾Ğ²Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚
        # EOS - ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ¼Ğ¾Ğ´ĞµĞ»Ñ– Ñ‰Ğ¾ Ñ‚ĞµĞºÑÑ‚ Ğ·Ğ°ĞºÑ–Ğ½Ñ‡Ğ¸Ğ²ÑÑ
        self.special_tokens = {
            "<PAD>": 0,
            "<UNK>": 1,
            "<BOS>": 2,
            "<EOS>": 3,
        }
        self.num_special = len(self.special_tokens)

        # Ğ¡Ğ»Ğ¾Ğ²Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ñ Ñ‚Ğ¾ĞºĞµĞ½ â†” Ñ–Ğ½Ğ´ĞµĞºÑ
        self.token_to_id = {}   # "Ğ¿Ñ€Ğ¸" â†’ 42
        self.id_to_token = {}   # 42 â†’ "Ğ¿Ñ€Ğ¸"

        # Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ·Ğ»Ğ¸Ñ‚Ñ‚Ñ–Ğ² BPE Ñƒ Ğ¿Ğ¾Ñ€ÑĞ´ĞºÑƒ Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ½Ğ½Ñ
        # ĞšĞ¾Ğ¶Ğ½Ğµ Ğ·Ğ»Ğ¸Ñ‚Ñ‚Ñ = (Ñ‚Ğ¾ĞºĞµĞ½_A, Ñ‚Ğ¾ĞºĞµĞ½_B) â†’ Ğ½Ğ¾Ğ²Ğ¸Ğ¹_Ñ‚Ğ¾ĞºĞµĞ½
        self.merges = []

        self.vocab_size = 0

    def _get_pairs(self, tokens):
        """
        Ğ—Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑƒÑÑ– Ğ¿Ğ°Ñ€Ğ¸ ÑÑƒÑÑ–Ğ´Ğ½Ñ–Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ² Ñ‚Ğ° Ñ€Ğ°Ñ…ÑƒÑ” Ñ—Ñ…Ğ½Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ.

        ĞĞ°Ğ¿Ñ€Ğ¸ĞºĞ»Ğ°Ğ´, Ğ´Ğ»Ñ ['Ğ¿', 'Ñ€', 'Ğ¸', 'Ğ²', 'Ñ–', 'Ñ‚']:
        Ğ¿Ğ°Ñ€Ğ¸: {('Ğ¿','Ñ€'): 1, ('Ñ€','Ğ¸'): 1, ('Ğ¸','Ğ²'): 1, ...}

        Ğ¦Ğµ ÑĞ´Ñ€Ğ¾ BPE â€” Ğ¼Ğ¸ Ğ·Ğ°Ğ²Ğ¶Ğ´Ğ¸ Ğ·Ğ»Ğ¸Ğ²Ğ°Ñ”Ğ¼Ğ¾ Ğ½Ğ°Ğ¹Ñ‡Ğ°ÑÑ‚Ñ–ÑˆÑƒ Ğ¿Ğ°Ñ€Ñƒ.
        """
        pairs = {}
        for i in range(len(tokens) - 1):
            pair = (tokens[i], tokens[i + 1])
            pairs[pair] = pairs.get(pair, 0) + 1
        return pairs

    def _merge_pair(self, tokens, pair):
        """
        Ğ—Ğ»Ğ¸Ğ²Ğ°Ñ” Ğ²ÑÑ– Ğ²Ñ…Ğ¾Ğ´Ğ¶ĞµĞ½Ğ½Ñ Ğ¿Ğ°Ñ€Ğ¸ (A, B) â†’ AB Ñƒ Ğ¿Ğ¾ÑĞ»Ñ–Ğ´Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ñ–.

        ĞĞ°Ğ¿Ñ€Ğ¸ĞºĞ»Ğ°Ğ´:
          tokens = ['Ğ¿', 'Ñ€', 'Ğ¸', 'Ğ²', 'Ñ–', 'Ñ‚']
          pair = ('Ğ¿', 'Ñ€')
          Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ = ['Ğ¿Ñ€', 'Ğ¸', 'Ğ²', 'Ñ–', 'Ñ‚']
        """
        new_tokens = []
        i = 0
        merged = pair[0] + pair[1]

        while i < len(tokens):
            if (i < len(tokens) - 1
                    and tokens[i] == pair[0]
                    and tokens[i + 1] == pair[1]):
                new_tokens.append(merged)
                i += 2
            else:
                new_tokens.append(tokens[i])
                i += 1

        return new_tokens

    def train(self, text, vocab_size=500):
        """
        ĞĞ°Ğ²Ñ‡Ğ°Ñ” BPE ÑĞ»Ğ¾Ğ²Ğ½Ğ¸Ğº Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ñ–.

        ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼:
        1. ĞŸĞ¾Ñ‡Ğ¸Ğ½Ğ°Ñ”Ğ¼Ğ¾ Ğ· Ğ°Ğ»Ñ„Ğ°Ğ²Ñ–Ñ‚Ñƒ Ğ¾ĞºÑ€ĞµĞ¼Ğ¸Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ–Ğ²
        2. ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑ”Ğ¼Ğ¾:
           a) Ğ Ğ°Ñ…ÑƒÑ”Ğ¼Ğ¾ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ ĞºĞ¾Ğ¶Ğ½Ğ¾Ñ— Ğ¿Ğ°Ñ€Ğ¸ ÑÑƒÑÑ–Ğ´Ğ½Ñ–Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ²
           b) Ğ—Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ğ½Ğ°Ğ¹Ñ‡Ğ°ÑÑ‚Ñ–ÑˆÑƒ Ğ¿Ğ°Ñ€Ñƒ
           c) Ğ—Ğ»Ğ¸Ğ²Ğ°Ñ”Ğ¼Ğ¾ Ñ†Ñ Ğ¿Ğ°Ñ€Ñƒ Ñƒ Ğ½Ğ¾Ğ²Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½
           d) Ğ”Ğ¾Ğ´Ğ°Ñ”Ğ¼Ğ¾ Ğ·Ğ»Ğ¸Ñ‚Ñ‚Ñ Ñƒ ÑĞ¿Ğ¸ÑĞ¾Ğº merges
        3. Ğ—ÑƒĞ¿Ğ¸Ğ½ÑÑ”Ğ¼Ğ¾ÑÑ ĞºĞ¾Ğ»Ğ¸ Ğ´Ğ¾ÑÑĞ³Ğ»Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¾Ğ³Ğ¾ vocab_size
        """
        print(f"ğŸ”¤ ĞĞ°Ğ²Ñ‡Ğ°Ğ½Ğ½Ñ BPE Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° (vocab_size={vocab_size})...")

        # â”€â”€ ĞšÑ€Ğ¾Ğº 1: ĞŸĞ¾Ñ‡Ğ°Ñ‚ĞºĞ¾Ğ²Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ²Ğ½Ğ¸Ğº Ğ· Ğ¾ĞºÑ€ĞµĞ¼Ğ¸Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ–Ğ² â”€â”€
        chars = sorted(set(text))
        self.token_to_id = dict(self.special_tokens)
        self.id_to_token = {v: k for k, v in self.special_tokens.items()}

        for ch in chars:
            idx = len(self.token_to_id)
            self.token_to_id[ch] = idx
            self.id_to_token[idx] = ch

        # â”€â”€ ĞšÑ€Ğ¾Ğº 2: Ğ Ğ¾Ğ·Ğ±Ğ¸Ğ²Ğ°Ñ”Ğ¼Ğ¾ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸ â”€â”€
        tokens = list(text)

        # â”€â”€ ĞšÑ€Ğ¾Ğº 3: Ğ†Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ»Ğ¸Ğ²Ğ°Ñ”Ğ¼Ğ¾ Ğ½Ğ°Ğ¹Ñ‡Ğ°ÑÑ‚Ñ–ÑˆÑ– Ğ¿Ğ°Ñ€Ğ¸ â”€â”€
        self.merges = []
        num_merges = vocab_size - len(self.token_to_id)

        for i in range(num_merges):
            pairs = self._get_pairs(tokens)
            if not pairs:
                break

            best_pair = max(pairs, key=pairs.get)
            merged_token = best_pair[0] + best_pair[1]
            tokens = self._merge_pair(tokens, best_pair)

            idx = len(self.token_to_id)
            self.token_to_id[merged_token] = idx
            self.id_to_token[idx] = merged_token
            self.merges.append(best_pair)

            if (i + 1) % 100 == 0:
                print(f"  Ğ—Ğ»Ğ¸Ñ‚Ñ‚Ñ {i+1}/{num_merges}: "
                      f"'{best_pair[0]}' + '{best_pair[1]}' â†’ '{merged_token}'")

        self.vocab_size = len(self.token_to_id)
        print(f"âœ… Ğ¡Ğ»Ğ¾Ğ²Ğ½Ğ¸Ğº Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ğ¹: {self.vocab_size} Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ²")

    def encode(self, text):
        """
        ĞŸĞµÑ€ĞµÑ‚Ğ²Ğ¾Ñ€ÑÑ” Ñ‚ĞµĞºÑÑ‚ Ñƒ Ğ¿Ğ¾ÑĞ»Ñ–Ğ´Ğ¾Ğ²Ğ½Ñ–ÑÑ‚ÑŒ ID Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ².

        ĞŸÑ€Ğ¾Ñ†ĞµÑ:
        1. Ğ Ğ¾Ğ·Ğ±Ğ¸Ğ²Ğ°Ñ”Ğ¼Ğ¾ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸
        2. ĞŸĞ¾ÑĞ»Ñ–Ğ´Ğ¾Ğ²Ğ½Ğ¾ Ğ·Ğ°ÑÑ‚Ğ¾ÑĞ¾Ğ²ÑƒÑ”Ğ¼Ğ¾ Ğ’Ğ¡Ğ† Ğ·Ğ»Ğ¸Ñ‚Ñ‚Ñ Ñƒ Ñ‚Ğ¾Ğ¼Ñƒ Ğ¶ Ğ¿Ğ¾Ñ€ÑĞ´ĞºÑƒ
           ÑĞº Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ½Ğ½Ñ– â€” Ñ†Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚ÑƒÑ” Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ñ–Ğ½Ğ¾Ğ²Ğ°Ğ½Ñ–ÑÑ‚ÑŒ
        3. ĞšĞ¾Ğ¶ĞµĞ½ Ñ‚Ğ¾ĞºĞµĞ½ â†’ Ğ¹Ğ¾Ğ³Ğ¾ ID
        """
        tokens = list(text)

        for pair in self.merges:
            tokens = self._merge_pair(tokens, pair)

        unk_id = self.special_tokens["<UNK>"]
        return [self.token_to_id.get(t, unk_id) for t in tokens]

    def decode(self, ids):
        """
        Ğ’Ñ–Ğ´Ğ½Ğ¾Ğ²Ğ»ÑÑ” Ñ‚ĞµĞºÑÑ‚ Ğ· Ğ¿Ğ¾ÑĞ»Ñ–Ğ´Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ñ– ID.
        ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ” ÑĞ¿ĞµÑ†Ñ–Ğ°Ğ»ÑŒĞ½Ñ– Ñ‚Ğ¾ĞºĞµĞ½Ğ¸.
        """
        tokens = []
        for idx in ids:
            token = self.id_to_token.get(idx, "")
            if token in self.special_tokens:
                continue
            tokens.append(token)
        return "".join(tokens)

    def save(self, path):
        """Ğ—Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ” Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ñƒ JSON Ñ„Ğ°Ğ¹Ğ»."""
        data = {
            "token_to_id": self.token_to_id,
            "merges": self.merges,
            "vocab_size": self.vocab_size,
        }
        os.makedirs(os.path.dirname(path) if os.path.dirname(path) else ".", exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ Ğ¢Ğ¾ĞºĞµĞ½Ñ–Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ·Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ¾: {path}")

    def load(self, path):
        """Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ÑƒÑ” Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ· JSON Ñ„Ğ°Ğ¹Ğ»Ñƒ."""
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)

        self.token_to_id = data["token_to_id"]
        self.id_to_token = {int(v): k for k, v in self.token_to_id.items()}
        self.merges = [tuple(m) for m in data["merges"]]
        self.vocab_size = data["vocab_size"]
        print(f"ğŸ“‚ Ğ¢Ğ¾ĞºĞµĞ½Ñ–Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ¾: {self.vocab_size} Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ²")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¢ĞµÑÑ‚ÑƒĞ²Ğ°Ğ½Ğ½Ñ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if __name__ == "__main__":
    text = open("data.txt", encoding="utf-8").read()
    tok = BPETokenizer()
    tok.train(text, vocab_size=200)

    sample = "Ğ¿Ñ€Ğ¸Ğ²Ñ–Ñ‚ ÑĞ²Ñ–Ñ‚"
    encoded = tok.encode(sample)
    decoded = tok.decode(encoded)

    print(f"\nĞ¢ĞµÑÑ‚:")
    print(f"  Ğ’Ñ…Ñ–Ğ´:      '{sample}'")
    print(f"  Encoded:   {encoded}")
    print(f"  Decoded:   '{decoded}'")
    print(f"  Ğ—Ğ±Ñ–Ğ³Ğ°Ñ”Ñ‚ÑŒÑÑ: {sample == decoded}")
